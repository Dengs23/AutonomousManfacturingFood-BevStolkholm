{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12c4dd0-9c53-4d51-8e84-b5ddcdac7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Creating autonomous manufacturing training data...\n",
      "‚úÖ Created 10000 samples\n",
      "üìä Failure rate: 13.6%\n",
      "üìà Safety risk average: 0.029\n",
      "üìà Quality risk average: 0.049\n",
      "üíæ Saved to training_data_autonomous.csv\n",
      "üíæ Test data saved to test_data_autonomous.csv\n"
     ]
    }
   ],
   "source": [
    "# create_training_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_autonomous_training_data(output_path='training_data_autonomous.csv'):\n",
    "    \"\"\"Create comprehensive training data for autonomous manufacturing\"\"\"\n",
    "    \n",
    "    print(\"üéØ Creating autonomous manufacturing training data...\")\n",
    "    \n",
    "    n_samples = 10000\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Base sensor readings (normal operation)\n",
    "    data = {\n",
    "        # Core maintenance sensors\n",
    "        'temperature_c': np.random.normal(75, 3, n_samples),\n",
    "        'pressure_bar': np.random.normal(15, 1.5, n_samples),\n",
    "        'ph_level': np.random.normal(6.8, 0.2, n_samples),\n",
    "        'flow_rate_lph': np.random.normal(1200, 100, n_samples),\n",
    "        \n",
    "        # Equipment health sensors\n",
    "        'vibration_x': np.random.exponential(0.5, n_samples),\n",
    "        'vibration_y': np.random.exponential(0.5, n_samples),\n",
    "        'vibration_z': np.random.exponential(0.5, n_samples),\n",
    "        'ultrasound_leak_db': np.random.exponential(5, n_samples),\n",
    "        'acoustic_emission': np.random.beta(2, 5, n_samples),\n",
    "        \n",
    "        # Quality & process sensors\n",
    "        'orp_redox_mv': np.random.normal(400, 50, n_samples),\n",
    "        'humidity_rh': np.random.normal(55, 8, n_samples),\n",
    "        'vision_defect_score': np.random.exponential(0.02, n_samples),\n",
    "        'vision_contaminant_score': np.random.exponential(0.002, n_samples),\n",
    "        \n",
    "        # Safety & environment sensors\n",
    "        'co2_ppm': np.random.normal(420, 100, n_samples),\n",
    "        'o2_percent': np.random.normal(20.8, 0.3, n_samples),\n",
    "        'voc_ppm': np.random.exponential(0.5, n_samples),\n",
    "        'differential_pressure_bar': np.random.exponential(0.05, n_samples),\n",
    "        'refrigerant_pressure_bar': np.random.normal(8.5, 0.5, n_samples),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ===== CREATE FAILURE SCENARIOS =====\n",
    "    failure_mask = np.zeros(n_samples, dtype=bool)\n",
    "    \n",
    "    # Scenario 1: Temperature spikes cause failures\n",
    "    temp_failures = df['temperature_c'] > 82\n",
    "    failure_mask = failure_mask | temp_failures\n",
    "    \n",
    "    # Scenario 2: High vibration causes failures\n",
    "    vib_magnitude = np.sqrt(df['vibration_x']**2 + df['vibration_y']**2 + df['vibration_z']**2)\n",
    "    vib_failures = vib_magnitude > 3.5\n",
    "    failure_mask = failure_mask | vib_failures\n",
    "    \n",
    "    # Scenario 3: Pressure spikes\n",
    "    pressure_failures = df['pressure_bar'] > 18\n",
    "    failure_mask = failure_mask | pressure_failures\n",
    "    \n",
    "    # Scenario 4: pH out of range (HACCP violation)\n",
    "    ph_failures = (df['ph_level'] < 5.5) | (df['ph_level'] > 7.5)\n",
    "    failure_mask = failure_mask | ph_failures\n",
    "    \n",
    "    # Scenario 5: Contamination detected\n",
    "    contaminant_failures = df['vision_contaminant_score'] > 0.01\n",
    "    failure_mask = failure_mask | contaminant_failures\n",
    "    \n",
    "    # Scenario 6: Safety hazards\n",
    "    safety_failures = (df['co2_ppm'] > 1000) | (df['voc_ppm'] > 8)\n",
    "    failure_mask = failure_mask | safety_failures\n",
    "    \n",
    "    # Create target variable\n",
    "    df['failure_risk'] = failure_mask.astype(int)\n",
    "    \n",
    "    # Add some noise (not all anomalies cause immediate failure)\n",
    "    noise = np.random.random(n_samples) < 0.1\n",
    "    df.loc[noise & failure_mask, 'failure_risk'] = 0\n",
    "    df.loc[noise & ~failure_mask, 'failure_risk'] = 1\n",
    "    \n",
    "    # Add derived features\n",
    "    df['vibration_magnitude'] = vib_magnitude\n",
    "    df['flow_rate_normalized'] = df['flow_rate_lph'] / 1200\n",
    "    df['temperature_deviation'] = abs(df['temperature_c'] - 75)\n",
    "    \n",
    "    # Calculate risk scores for training\n",
    "    df['safety_risk_score'] = (\n",
    "        (df['co2_ppm'] > 800).astype(int) * 0.4 +\n",
    "        (df['voc_ppm'] > 5).astype(int) * 0.3 +\n",
    "        (df['pressure_bar'] > 17).astype(int) * 0.3\n",
    "    )\n",
    "    \n",
    "    df['quality_risk_score'] = (\n",
    "        ((df['ph_level'] < 5.5) | (df['ph_level'] > 7.5)).astype(int) * 0.4 +\n",
    "        (df['vision_defect_score'] > 0.05).astype(int) * 0.3 +\n",
    "        (df['vision_contaminant_score'] > 0.005).astype(int) * 0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created {n_samples} samples\")\n",
    "    print(f\"üìä Failure rate: {df['failure_risk'].mean():.1%}\")\n",
    "    print(f\"üìà Safety risk average: {df['safety_risk_score'].mean():.3f}\")\n",
    "    print(f\"üìà Quality risk average: {df['quality_risk_score'].mean():.3f}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved to {output_path}\")\n",
    "    \n",
    "    # Create a smaller test dataset\n",
    "    test_df = df.sample(1000, random_state=42)\n",
    "    test_df.to_csv('test_data_autonomous.csv', index=False)\n",
    "    print(f\"üíæ Test data saved to test_data_autonomous.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = create_autonomous_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522aabb5-f0e8-4e62-a9d4-41d107841e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ TRAINING AUTONOMOUS MANUFACTURING MODEL\n",
      "============================================================\n",
      "üì• Loaded 10000 samples from training_data_autonomous.csv\n",
      "üîß Using 21 features for training\n",
      "   Features: ['temperature_c', 'pressure_bar', 'ph_level', 'flow_rate_lph', 'vibration_x', 'vibration_y', 'vibration_z', 'ultrasound_leak_db', 'acoustic_emission', 'orp_redox_mv', 'humidity_rh', 'vision_defect_score', 'vision_contaminant_score', 'co2_ppm', 'o2_percent', 'voc_ppm', 'differential_pressure_bar', 'refrigerant_pressure_bar', 'vibration_magnitude', 'flow_rate_normalized', 'temperature_deviation']\n",
      "\n",
      "üìä Data split:\n",
      "   Training samples: 8000\n",
      "   Test samples: 2000\n",
      "   Failure rate in training: 13.55%\n",
      "   Failure rate in test: 13.55%\n",
      "\n",
      "‚öôÔ∏è  Scaling features...\n",
      "üéØ Training RandomForest model...\n",
      "\n",
      "üìà Evaluating model performance...\n",
      "‚úÖ Test Accuracy: 0.900\n",
      "‚úÖ Cross-validation AUC: 0.641 (¬±0.026)\n",
      "\n",
      "üîù Top 10 most important features:\n",
      "   pressure_bar: 0.139\n",
      "   vision_contaminant_score: 0.076\n",
      "   temperature_c: 0.069\n",
      "   temperature_deviation: 0.048\n",
      "   vibration_magnitude: 0.044\n",
      "   differential_pressure_bar: 0.043\n",
      "   ultrasound_leak_db: 0.042\n",
      "   vibration_z: 0.041\n",
      "   vibration_y: 0.041\n",
      "   ph_level: 0.040\n",
      "\n",
      "üíæ Saving model package...\n",
      "‚úÖ Model saved to autonomous_model_package/autonomous_model.joblib\n",
      "‚úÖ Scaler saved to autonomous_model_package/scaler.joblib\n",
      "‚úÖ Features metadata saved to autonomous_model_package/features.json\n",
      "‚úÖ Inference script saved to autonomous_model_package/production_inference.py\n",
      "\n",
      "üì¶ Creating SageMaker deployment package...\n",
      "‚úÖ Created model.tar.gz (7.7 MB)\n",
      "\n",
      "üß™ Testing model locally...\n",
      "\n",
      "üìä Test prediction:\n",
      "   Prediction: NORMAL\n",
      "   Failure probability: 0.225\n",
      "   Confidence: 0.775\n",
      "‚úÖ Example payload saved to autonomous_model_package/example_payload.json\n",
      "\n",
      "============================================================\n",
      "üéâ AUTONOMOUS MANUFACTURING MODEL TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä Model Performance: 90.0% accuracy\n",
      "üìÅ Model package: autonomous_model_package/\n",
      "üì¶ Deployment package: model.tar.gz\n",
      "üîß Features: 21\n",
      "\n",
      "üöÄ Next steps:\n",
      "   1. Copy production_inference.py into autonomous_model_package/\n",
      "   2. Upload model.tar.gz to S3\n",
      "   3. Deploy to SageMaker endpoint\n",
      "   4. Update Lambda to use new endpoint\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# train_autonomous_model.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def train_autonomous_model():\n",
    "    \"\"\"Train the complete autonomous manufacturing model\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ TRAINING AUTONOMOUS MANUFACTURING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load or create training data\n",
    "    try:\n",
    "        df = pd.read_csv('training_data_autonomous.csv')\n",
    "        print(f\"üì• Loaded {len(df)} samples from training_data_autonomous.csv\")\n",
    "    except:\n",
    "        print(\"üìù Creating new training data...\")\n",
    "        from create_training_data import create_autonomous_training_data\n",
    "        df = create_autonomous_training_data()\n",
    "    \n",
    "    # ===== DEFINE ALL FEATURES =====\n",
    "    base_features = [\n",
    "        # Core maintenance\n",
    "        'temperature_c', 'pressure_bar', 'ph_level', 'flow_rate_lph',\n",
    "        \n",
    "        # Equipment health\n",
    "        'vibration_x', 'vibration_y', 'vibration_z',\n",
    "        'ultrasound_leak_db', 'acoustic_emission',\n",
    "        \n",
    "        # Quality & process\n",
    "        'orp_redox_mv', 'humidity_rh',\n",
    "        'vision_defect_score', 'vision_contaminant_score',\n",
    "        \n",
    "        # Safety & environment\n",
    "        'co2_ppm', 'o2_percent', 'voc_ppm',\n",
    "        'differential_pressure_bar', 'refrigerant_pressure_bar'\n",
    "    ]\n",
    "    \n",
    "    # Add derived features if they exist\n",
    "    derived_features = ['vibration_magnitude', 'flow_rate_normalized', 'temperature_deviation']\n",
    "    available_derived = [f for f in derived_features if f in df.columns]\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = base_features + available_derived\n",
    "    print(f\"üîß Using {len(all_features)} features for training\")\n",
    "    print(f\"   Features: {all_features}\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = df[all_features].fillna(0)\n",
    "    y = df['failure_risk']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Data split:\")\n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "    print(f\"   Failure rate in training: {y_train.mean():.2%}\")\n",
    "    print(f\"   Failure rate in test: {y_test.mean():.2%}\")\n",
    "    \n",
    "    # Scale features\n",
    "    print(\"\\n‚öôÔ∏è  Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"üéØ Training RandomForest model...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # ===== EVALUATE MODEL =====\n",
    "    print(\"\\nüìà Evaluating model performance...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    print(f\"‚úÖ Test Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"‚úÖ Cross-validation AUC: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîù Top 10 most important features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    # ===== SAVE MODEL & ARTIFACTS =====\n",
    "    print(\"\\nüíæ Saving model package...\")\n",
    "    \n",
    "    # Create model directory\n",
    "    import os\n",
    "    model_dir = 'autonomous_model_package'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save the model\n",
    "    model_path = os.path.join(model_dir, 'autonomous_model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"‚úÖ Model saved to {model_path}\")\n",
    "    \n",
    "    # 2. Save the scaler\n",
    "    scaler_path = os.path.join(model_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"‚úÖ Scaler saved to {scaler_path}\")\n",
    "    \n",
    "    # 3. Save feature metadata\n",
    "    features_metadata = {\n",
    "        'features': all_features,\n",
    "        'feature_importance': feature_importance.to_dict('records'),\n",
    "        'training_date': pd.Timestamp.now().isoformat(),\n",
    "        'model_performance': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'auc_mean': float(cv_scores.mean()),\n",
    "            'auc_std': float(cv_scores.std()),\n",
    "            'n_training_samples': len(X_train),\n",
    "            'n_features': len(all_features)\n",
    "        },\n",
    "        'default_values': {\n",
    "            'temperature_c': 75.0,\n",
    "            'pressure_bar': 15.0,\n",
    "            'ph_level': 6.8,\n",
    "            'flow_rate_lph': 1200.0,\n",
    "            'vibration_x': 0.0,\n",
    "            'vibration_y': 0.0,\n",
    "            'vibration_z': 0.0,\n",
    "            'ultrasound_leak_db': 0.0,\n",
    "            'acoustic_emission': 0.0,\n",
    "            'orp_redox_mv': 400.0,\n",
    "            'humidity_rh': 55.0,\n",
    "            'vision_defect_score': 0.0,\n",
    "            'vision_contaminant_score': 0.0,\n",
    "            'co2_ppm': 420.0,\n",
    "            'o2_percent': 20.8,\n",
    "            'voc_ppm': 0.0,\n",
    "            'differential_pressure_bar': 0.1,\n",
    "            'refrigerant_pressure_bar': 8.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    features_path = os.path.join(model_dir, 'features.json')\n",
    "    with open(features_path, 'w') as f:\n",
    "        json.dump(features_metadata, f, indent=2)\n",
    "    print(f\"‚úÖ Features metadata saved to {features_path}\")\n",
    "    \n",
    "    # 4. Save production_inference.py\n",
    "    inference_code = '''\n",
    "# production_inference.py - Autonomous Manufacturing Model\n",
    "# [PASTE THE ENTIRE production_inference.py CODE HERE]\n",
    "'''\n",
    "    \n",
    "    inference_path = os.path.join(model_dir, 'production_inference.py')\n",
    "    with open(inference_path, 'w') as f:\n",
    "        # You'll need to copy the production_inference.py code here\n",
    "        # For now, we'll create a placeholder\n",
    "        f.write(\"# Autonomous Manufacturing Inference Script\\n\")\n",
    "        f.write(\"# Replace with the full production_inference.py code\\n\")\n",
    "    print(f\"‚úÖ Inference script saved to {inference_path}\")\n",
    "    \n",
    "    # 5. Create model.tar.gz for SageMaker\n",
    "    print(\"\\nüì¶ Creating SageMaker deployment package...\")\n",
    "    import tarfile\n",
    "    \n",
    "    # Create tar.gz archive\n",
    "    with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "        tar.add(model_dir, arcname='.')\n",
    "    \n",
    "    print(f\"‚úÖ Created model.tar.gz ({os.path.getsize('model.tar.gz') / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    # 6. Test the model locally\n",
    "    print(\"\\nüß™ Testing model locally...\")\n",
    "    \n",
    "    # Create a test sample\n",
    "    test_sample = {\n",
    "        'temperature_c': 79.5,\n",
    "        'pressure_bar': 15.2,\n",
    "        'ph_level': 6.5,\n",
    "        'flow_rate_lph': 1250,\n",
    "        'vibration_x': 2.3,\n",
    "        'vibration_y': 1.8,\n",
    "        'vibration_z': 3.1,\n",
    "        'ultrasound_leak_db': 45.2,\n",
    "        'acoustic_emission': 0.85,\n",
    "        'orp_redox_mv': 450,\n",
    "        'humidity_rh': 65.2,\n",
    "        'vision_defect_score': 0.02,\n",
    "        'vision_contaminant_score': 0.001,\n",
    "        'co2_ppm': 850,\n",
    "        'o2_percent': 20.8,\n",
    "        'voc_ppm': 2.1,\n",
    "        'differential_pressure_bar': 0.15,\n",
    "        'refrigerant_pressure_bar': 8.2\n",
    "    }\n",
    "    \n",
    "    # Prepare features in correct order\n",
    "    X_test_sample = np.array([[test_sample.get(f, 0) for f in all_features]])\n",
    "    X_test_scaled_sample = scaler.transform(X_test_sample)\n",
    "    \n",
    "    prediction = model.predict(X_test_scaled_sample)[0]\n",
    "    probability = model.predict_proba(X_test_scaled_sample)[0]\n",
    "    \n",
    "    print(f\"\\nüìä Test prediction:\")\n",
    "    print(f\"   Prediction: {'FAILURE' if prediction == 1 else 'NORMAL'}\")\n",
    "    print(f\"   Failure probability: {probability[1]:.3f}\")\n",
    "    print(f\"   Confidence: {np.max(probability):.3f}\")\n",
    "    \n",
    "    # Create example payload\n",
    "    example_payload = {\n",
    "        'features': all_features,\n",
    "        'example_input': test_sample,\n",
    "        'expected_output': {\n",
    "            'prediction': int(prediction),\n",
    "            'probability_failure': float(probability[1]),\n",
    "            'confidence': float(np.max(probability))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    example_path = os.path.join(model_dir, 'example_payload.json')\n",
    "    with open(example_path, 'w') as f:\n",
    "        json.dump(example_payload, f, indent=2)\n",
    "    print(f\"‚úÖ Example payload saved to {example_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ AUTONOMOUS MANUFACTURING MODEL TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìä Model Performance: {accuracy:.1%} accuracy\")\n",
    "    print(f\"üìÅ Model package: {model_dir}/\")\n",
    "    print(f\"üì¶ Deployment package: model.tar.gz\")\n",
    "    print(f\"üîß Features: {len(all_features)}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Next steps:\")\n",
    "    print(f\"   1. Copy production_inference.py into {model_dir}/\")\n",
    "    print(f\"   2. Upload model.tar.gz to S3\")\n",
    "    print(f\"   3. Deploy to SageMaker endpoint\")\n",
    "    print(f\"   4. Update Lambda to use new endpoint\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, scaler, all_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler, features = train_autonomous_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3619e4-5086-467d-be8f-6d461ed6ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGEMAKER_ENDPOINT_NAME = \"autonomous-manufacturing-v1\"  # Changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7780e5-8941-44e8-926b-8b6df1f484e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ DEPLOYING WITH CORRECT ECR IMAGE\n",
      "============================================================\n",
      "üì¶ Model: s3://sagemaker-eu-north-1-976792586723/models/final-model-v2.tar.gz\n",
      "üè∑Ô∏è  Endpoint: autonomous-manufacturing-v1\n",
      "üê≥ ECR Image: 763104351884.dkr.ecr.eu-north-1.amazonaws.com/sklearn:0.23-1-cpu-py3\n",
      "\n",
      "1. Creating model...\n",
      "\n",
      "‚ùå Error: An error occurred (UnrecognizedClientException) when calling the CreateModel operation: The security token included in the request is invalid.\n"
     ]
    }
   ],
   "source": [
    "# Updated deployment script with correct image\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def deploy_with_fixed_image():\n",
    "    \"\"\"Deployment with correct ECR image\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ DEPLOYING WITH CORRECT ECR IMAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Configuration\n",
    "    MODEL_S3 = \"s3://sagemaker-eu-north-1-976792586723/models/final-model-v2.tar.gz\" \n",
    "    ENDPOINT_NAME = \"autonomous-manufacturing-v1\"\n",
    "    REGION = \"eu-north-1\"\n",
    "    ROLE_ARN = \"arn:aws:iam::976792586723:role/AmazonSageMaker-ExecutionRole-20260207T095196\"\n",
    "    \n",
    "    # CORRECT IMAGE FOR eu-north-1 REGION\n",
    "    ECR_IMAGE = \"763104351884.dkr.ecr.eu-north-1.amazonaws.com/sklearn:0.23-1-cpu-py3\"\n",
    "    \n",
    "    print(f\"üì¶ Model: {MODEL_S3}\")\n",
    "    print(f\"üè∑Ô∏è  Endpoint: {ENDPOINT_NAME}\")\n",
    "    print(f\"üê≥ ECR Image: {ECR_IMAGE}\")\n",
    "    \n",
    "    sm = boto3.client('sagemaker', region_name=REGION)\n",
    "    \n",
    "    try:\n",
    "        # Create Model\n",
    "        print(\"\\n1. Creating model...\")\n",
    "        sm.create_model(\n",
    "            ModelName='autonomous-manufacturing-model',\n",
    "            ExecutionRoleArn=ROLE_ARN,\n",
    "            PrimaryContainer={\n",
    "                'Image': ECR_IMAGE,  # FIXED IMAGE\n",
    "                'ModelDataUrl': MODEL_S3,\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_PROGRAM': 'production_inference.py'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(\"   ‚úÖ Model created\")\n",
    "        \n",
    "        # Rest of your code...\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deploy_with_fixed_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3643b166-b9be-4b86-bf1d-11ea7d413e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "üöÄ TRYING SAGEMAKER SDK APPROACH\n",
      "üì¶ Creating model with SDK...\n",
      "‚ùå SDK failed: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "\n",
    "print(\"üöÄ TRYING SAGEMAKER SDK APPROACH\")\n",
    "\n",
    "# Initialize session\n",
    "session = Session(boto_session=boto3.Session(region_name='eu-north-1'))\n",
    "\n",
    "# Create model using SDK\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=\"s3://sagemaker-eu-north-1-976792586723/models/final-model-v2.tar.gz\",\n",
    "    role=\"arn:aws:iam::976792586723:role/AmazonSageMaker-ExecutionRole-20260207T095196\",\n",
    "    entry_point=\"production_inference.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"üì¶ Creating model with SDK...\")\n",
    "    # Just create model first (not deploy)\n",
    "    sklearn_model.create(\n",
    "        instance_type='ml.m5.large',\n",
    "        accelerator_type=None\n",
    "    )\n",
    "    print(\"‚úÖ Model created with SDK!\")\n",
    "    \n",
    "    # Now deploy\n",
    "    print(\"üöÄ Deploying endpoint...\")\n",
    "    predictor = sklearn_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name='autonomous-manufacturing-sdk-test'\n",
    "    )\n",
    "    print(\"üéâ DEPLOYMENT SUCCESSFUL!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SDK failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ec98e-c0b7-4b8a-a112-af44d39a2211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac26e88-b6b6-4d10-a8c3-646f46dd4168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
